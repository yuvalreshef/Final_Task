{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is_C-pwyrp54"
   },
   "source": [
    "# Final Assignment - Implementing VAT\n",
    "Implementing stage 1 and evaluating as requested in stage 4.\n",
    "\n",
    "> `By: Yuval Rehsef`\n",
    "\n",
    "> `ID: 314805045`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PRcE_mfrvhK"
   },
   "source": [
    "## Import Libraries\n",
    "**`note: set the path to the datasets folder here`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK3rJUnkepsA",
    "outputId": "5599e5af-34bf-418e-a767-6340ba8b1e0f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, precision_score, auc, confusion_matrix, make_scorer, accuracy_score\n",
    "from scipy.stats import uniform, norm\n",
    "from skorch import NeuralNetClassifier\n",
    "import skorch\n",
    "from skorch.utils import to_tensor\n",
    "from skorch.callbacks import Checkpoint\n",
    "import pandas as pd\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "\n",
    "data_path = Path('datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctsy2Mh-rx_s"
   },
   "source": [
    "## Load Datasets\n",
    "`Loading the datasets and preprocessing with sklearns StandardScaler.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "qLLPn3hagBcl",
    "outputId": "28b9032a-f705-48ab-f255-ca561bd4973c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='20' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [20/20 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = {}\n",
    "for f in progress_bar(list(data_path.glob('*.csv'))):\n",
    "    df = pd.read_csv(f)\n",
    "    X = df[df.columns[:-1]]\n",
    "    y = df[df.columns[-1]]\n",
    "    if y.dtype != np.int64:\n",
    "        y = y.replace({'N':0, 'P':1}) # only one datasets label is not an integer, deal with it\n",
    "    y = np.eye(y.max() + 1)[y.values]\n",
    "    dfs[f.stem] = (X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4jlK1y8pwpQ"
   },
   "source": [
    "## VAT Implementation\n",
    "`Implementing the VAT loss and additional necessary functions.`\n",
    "\n",
    "`Link to the official implementation:`\n",
    "[GitHub link](https://github.com/lyakaap/VAT-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uvJ8xYBRpwv4"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def _disable_tracking_bn_stats(model):\n",
    "\n",
    "    def switch_attr(m):\n",
    "        if hasattr(m, 'track_running_stats'):\n",
    "            m.track_running_stats ^= True\n",
    "            \n",
    "    model.apply(switch_attr)\n",
    "    yield\n",
    "    model.apply(switch_attr)\n",
    "\n",
    "\n",
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
    "    return d\n",
    "\n",
    "\n",
    "class VATLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=1.0, xi=1e-6, eps=1.0, ip=1):\n",
    "        \"\"\"VAT loss\n",
    "        :param xi: hyperparameter of VAT (default: 10.0)\n",
    "        :param eps: hyperparameter of VAT (default: 1.0)\n",
    "        :param ip: iteration times of computing adv noise (default: 1)\n",
    "        \"\"\"\n",
    "        super(VATLoss, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.ip = ip\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, model, x):\n",
    "        with torch.no_grad():\n",
    "            pred = F.softmax(model(x), dim=1)\n",
    "\n",
    "        # prepare random unit tensor\n",
    "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
    "        d = _l2_normalize(d)\n",
    "\n",
    "        with _disable_tracking_bn_stats(model):\n",
    "            # calc adversarial direction\n",
    "            for _ in range(self.ip):\n",
    "                d.requires_grad_()\n",
    "                pred_hat = model(x + self.xi * d)\n",
    "                logp_hat = F.log_softmax(pred_hat, dim=1)\n",
    "                adv_distance = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
    "                adv_distance.backward()\n",
    "                d = _l2_normalize(d.grad)\n",
    "                model.zero_grad()\n",
    "    \n",
    "            # calc LDS\n",
    "            r_adv = d * self.eps\n",
    "            pred_hat = model(x + r_adv)\n",
    "            logp_hat = F.log_softmax(pred_hat, dim=1)\n",
    "            lds = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
    "\n",
    "        return self.alpha*lds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnUu-M5X1WBV"
   },
   "source": [
    "## Model Class\n",
    "\n",
    "`Setting up the pytorch model and the skorch object. Skorch is a wrapper for pytorch for using pytorch model as sklearn model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EMDcmuP31WMa"
   },
   "outputs": [],
   "source": [
    "class VATNet(nn.Module):\n",
    "    def __init__(self, input_size, classes): \n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                                    nn.Linear(input_size, 64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(16, classes)\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MySkorch(skorch.NeuralNet):\n",
    "    def train_step_single(self, x, y, **fit_params):\n",
    "        self.module_.train()\n",
    "        x, y = to_tensor((x.float(), y.long()), device=self.device)\n",
    "        y_pred = self.module_(x)\n",
    "        ce_loss = F.cross_entropy(y_pred, y)\n",
    "        vat_loss = self.criterion_(self.module_, x)\n",
    "        loss = ce_loss + vat_loss\n",
    "        loss.backward()\n",
    "        return {'loss': loss, 'y_pred': y_pred}\n",
    "\n",
    "    def validation_step(self, x, y, **fit_params):\n",
    "        self.module_.eval()\n",
    "        x, y = to_tensor((x.float(), y.long()), device=self.device)\n",
    "        y_pred = self.module_(x)\n",
    "        ce_loss = F.cross_entropy(y_pred, y)\n",
    "        vat_loss = self.criterion_(self.module_, x)\n",
    "        loss = ce_loss + vat_loss\n",
    "        return {'loss': loss, 'y_pred': y_pred}\n",
    "    \n",
    "    def evaluation_step(self, x, training=False):\n",
    "        self.check_is_fitted()\n",
    "        x = to_tensor(x.float(), device=self.device)\n",
    "        with torch.set_grad_enabled(training):\n",
    "            self.module_.train(training)\n",
    "            return F.softmax(self.module_(x), dim=1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ELZ987bPNNX"
   },
   "source": [
    "## Create Pipeline\n",
    "`Define the skorch model & functions to be used by a sklearn pipeline.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A_ELgyAcPNUE"
   },
   "outputs": [],
   "source": [
    "def make_VAT_loss(alpha=1.0, VAT_eps=2.0, VAT_xi=10.0, VAT_ip=1):\n",
    "    VAT_loss = VATLoss(alpha=alpha, xi=VAT_xi, eps=VAT_eps, ip=VAT_ip)\n",
    "    return VAT_loss\n",
    "\n",
    "def my_auc(y_true, y_pred):  \n",
    "    return roc_auc_score(y_true, y_pred, average='macro', multi_class='ovo')\n",
    "\n",
    "def get_scores(model, x, y):\n",
    "    preds = model.predict(x)\n",
    "    proba = model.predict_proba(x)\n",
    "    cm = confusion_matrix(y.argmax(1), preds)\n",
    "\n",
    "    if len(np.unique(y.argmax(1))) == 2:\n",
    "        fp = cm[0,1]\n",
    "        fn = cm[1,0]\n",
    "        tp = cm[1,1]\n",
    "        tn = cm[0,0]\n",
    "        auc_res = roc_auc_score(y.argmax(1), proba[:,1], average='macro', multi_class='ovo')\n",
    "    else:\n",
    "        fp = (cm.sum(axis=0) - np.diag(cm)).astype(float)\n",
    "        fn = (cm.sum(axis=1) - np.diag(cm)).astype(float)\n",
    "        tp = np.diag(cm).astype(float)\n",
    "        tn = (cm.sum() - (fp + fn + tp)).astype(float)\n",
    "        auc_res = roc_auc_score(y.argmax(1), proba, average='macro', multi_class='ovo')\n",
    "\n",
    "    tpr = tp/(tp+fn)\n",
    "    fpr = fp/(fp+tn)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y.ravel(), proba.ravel())\n",
    "\n",
    "    return {'Accuracy': accuracy_score(y.argmax(1), preds),\n",
    "            'TPR': tpr.mean(),\n",
    "            'FPR': fpr.mean(),\n",
    "            'Precision': precision_score(y.argmax(1), preds, average='macro'),\n",
    "            'AUC': auc_res,\n",
    "            'PR Curve': auc(recall, precision)\n",
    "    }\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "hypers = {\n",
    "    'net__lr':uniform(loc=1e-4, scale=(1e-2 - 1e-4)),\n",
    "    'net__criterion__VAT_eps':uniform(loc=1, scale=2),\n",
    "    'net__criterion__alpha':uniform(loc=0.5, scale=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llnPuE6foBoO"
   },
   "source": [
    "## Evaluation Over the Nested Cross-Validation\n",
    "`Evaluating the model performance over the 20 datasets, saving and printing the results.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "-Mkv0W-koBBD",
    "outputId": "81fcff88-0f81-42b3-bdc8-2601ca089fc0"
   },
   "outputs": [],
   "source": [
    "scores_dfs = pd.DataFrame(columns=['Dataset Name', 'Algorithm Name', 'Cross Validation', 'Hyperparameters Values', \n",
    "                                   'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR Curve', 'Training Time', 'Inference Time'])\n",
    "\n",
    "displayer = display(scores_dfs, display_id=True, clear=True)\n",
    "for data_name, (X,y) in progress_bar(dfs.items()):\n",
    "    print(data_name)\n",
    "    best_score = 0\n",
    "    X = X.to_numpy()\n",
    "    cv_counter=0\n",
    "    for train_index, test_index in progress_bar(outer_cv.split(X,y.argmax(1)), total=10):\n",
    "        cv_counter += 1\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        if y_train.argmax(1).min() == 1:\n",
    "            y_train = y_train.argmax(1) - 1\n",
    "            y_test = y_test[:, 1:]\n",
    "        else:\n",
    "            y_train = y_train.argmax(1)\n",
    "\n",
    "        net = MySkorch(VATNet, max_epochs=10, criterion=make_VAT_loss, module__input_size=x_train.shape[-1], module__classes=y_train.max() + 1, \n",
    "                       optimizer=optim.Adam, batch_size=32, train_split=None, device='cpu', verbose=0)\n",
    "        pipe = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('net', net)\n",
    "        ])\n",
    "        \n",
    "        rsg = RandomizedSearchCV(pipe, hypers, n_iter=50, n_jobs=-1, scoring=make_scorer(accuracy_score), cv=inner_cv, random_state=0)\n",
    "        \n",
    "        train_start = datetime.now()\n",
    "        rsg.fit(X=x_train, y=y_train)\n",
    "        train_time = (datetime.now() - train_start)\n",
    "        \n",
    "        if train_time.seconds == 0:\n",
    "            train_time = f'{train_time.microseconds}ms'\n",
    "        else:\n",
    "            train_time = f'{train_time.seconds}s'\n",
    "\n",
    "        inference_indexes = np.random.choice(range(len(x_train)), size=(1000,))\n",
    "        inference_start = datetime.now()\n",
    "        _ = rsg.predict(x_train[inference_indexes])\n",
    "        inference_time = (datetime.now() - inference_start)\n",
    "\n",
    "        if inference_time.seconds == 0:\n",
    "            inference_time = f'{inference_time.microseconds}ms'\n",
    "        else:\n",
    "            inference_time = f'{inference_time.seconds}s'\n",
    "\n",
    "        results = get_scores(rsg, x_test, y_test)\n",
    "        results['Dataset Name'] = data_name\n",
    "        results['Algorithm Name'] = 'VAT'\n",
    "        results['Cross Validation'] = cv_counter\n",
    "        results['Hyperparameters Values'] = rsg.best_params_\n",
    "        results['Training Time'] = train_time\n",
    "        results['Inference Time'] = inference_time\n",
    "\n",
    "        scores_dfs = scores_dfs.append(results, ignore_index=True)\n",
    "        displayer.update(scores_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Saving the results DataFrame.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dfs.to_csv('results/VAT_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "final_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
